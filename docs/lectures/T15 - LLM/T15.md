---
marp: true
theme: default
class:
  - invert
author: Nirand Pisutha-Arnond
paginate: true
footer: "255411: Information Technologies for Industrial Engineers"
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Prompt:ital,wght@0,100;0,300;0,400;0,700;1,100;1,300;1,400;1,700&display=swap');

    :root {
    font-family: Prompt;
    --hl-color: #D57E7E;
}
h1 {
  font-family: Prompt
}
</style>

# Information Technologies for Industrial Engineers

## เทคโนโลยีสารสนเทศสำหรับวิศวกรอุตสาหการ

---

# Large language model (LLM)

---

# LLM

- A type of advanced software that can communicate in a human-like manner.
- General-purpose language processing models
  - Pre-trained on extensive datasets covering a wide range of topics.
  - Understand the fundamental structures and semantics of human language.
- `Large`
  - Substantial amount training data
  - Billions or even trillions of parameters

---

![](./img/history.png)

---

# Chatbots and rule-based systems (1960s)

- `ELIZA`
  - The first chatbot ever built by humans.
- It can create an illusion of a conversation
  - by rephrasing user statements as questions using pattern matching and substitution methodology.
- [Try it.](https://web.njit.edu/~ronkowit/eliza.html)

---

# Recurrent Neural Networks (1980s)

- Neural networks that can "remember" previous input.
  - Using feedback loop.
- Suitable for natural language processing (NLP) tasks.
- Still, they are not good at retaining memory and suffer from long term memory loss.
  - Vanishing gradient.

---

# Long Short Term Memory (1990s)

- Specialised type of RNN.
- Can remember information over long sequences.
- LSTM architecture
  - `input`, `forget`, `output` gates
  - These gates determined how much information should be memorised, discarded, or output at each step.

---

# Gated Recurrent Network (2010s)

- Designed to solve some of the same problems as LSTMs
  - but with a simple and more streamlined structure.
- GRUs architecture
  - `update` and `reset` gate
  - The reduced gating in GRUs made them more efficient in terms of computation.

---

# Attention Mechanism (2014)

- RNN based variants LSTM and GRU are not great at retaining the context when it was far away.
- Attention allows the model to look back to the entire source sequence dynamically
  - Selecting different parts based on their relevance at each step of the output.
- Performs much better especially in longer sequences.

---

# Transformers Architecture (2017)
